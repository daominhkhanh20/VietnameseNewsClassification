{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "be17ff7b36b24d108c0582308a892914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bf4a06bdabfc428996f5d31eee2f3672",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7314357dc03944b596e771c645c70b90",
              "IPY_MODEL_8d8ffaaeced04692a4844481bac1351d"
            ]
          }
        },
        "bf4a06bdabfc428996f5d31eee2f3672": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7314357dc03944b596e771c645c70b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1972e9e6a69f468f8444cde1d74d298d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 557,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 557,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff8a6bec9adf4b41a029b9b35b59734c"
          }
        },
        "8d8ffaaeced04692a4844481bac1351d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_19d5d673346f41e5855d4a138a95be6a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 557/557 [00:00&lt;00:00, 1.90kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aa0d075a5f154b21979a572a62d21b22"
          }
        },
        "1972e9e6a69f468f8444cde1d74d298d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff8a6bec9adf4b41a029b9b35b59734c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19d5d673346f41e5855d4a138a95be6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aa0d075a5f154b21979a572a62d21b22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb97bca2b8c04620a0f12c4338847a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b67ad8c5c148422a979bb883fb97b931",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6fd2fe40dd274e7db9842ae91f83b31f",
              "IPY_MODEL_0b68efa816844299a0cc425f9b82793d"
            ]
          }
        },
        "b67ad8c5c148422a979bb883fb97b931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6fd2fe40dd274e7db9842ae91f83b31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dc776ee590394dc8b31652ea3684433b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 542923308,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 542923308,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ba9749fa8f74e5b8b742b7a383fb8f4"
          }
        },
        "0b68efa816844299a0cc425f9b82793d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a0cc5516341640379d7f2507ed6b26a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 543M/543M [00:17&lt;00:00, 31.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9ab8349d7f243619bf9aa64602186b0"
          }
        },
        "dc776ee590394dc8b31652ea3684433b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ba9749fa8f74e5b8b742b7a383fb8f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0cc5516341640379d7f2507ed6b26a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9ab8349d7f243619bf9aa64602186b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa4f5189f341468990b240395a1aef47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_874b2b6f092449d3ab0f357ec076a9ea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_82757dd96fc6482886dbe7bb8ce824dd",
              "IPY_MODEL_846f4f79cdee41ebaec9ae22c671f8d6"
            ]
          }
        },
        "874b2b6f092449d3ab0f357ec076a9ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82757dd96fc6482886dbe7bb8ce824dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8be834c52eb14221a6daa2b0ae6b05f2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 895321,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 895321,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_852aac3edecd429899c947966252664e"
          }
        },
        "846f4f79cdee41ebaec9ae22c671f8d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6443d0372e664849b0576e543f6ec3c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 895k/895k [00:00&lt;00:00, 1.83MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d95e41450a2411796758a055087e733"
          }
        },
        "8be834c52eb14221a6daa2b0ae6b05f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "852aac3edecd429899c947966252664e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6443d0372e664849b0576e543f6ec3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d95e41450a2411796758a055087e733": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26db5ac105514318a034a4f3e8a329fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ef4a537c04c34f98bd9012870cf15e19",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_92ca5802b14844f18f17128836b83541",
              "IPY_MODEL_30c8c82af4ad4f988425422c71e8c0ba"
            ]
          }
        },
        "ef4a537c04c34f98bd9012870cf15e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92ca5802b14844f18f17128836b83541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_405bd06719854c1380c38cb71c3ec9aa",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1135173,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1135173,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7bf8f690f2a443ccbe2d253c1e7cabca"
          }
        },
        "30c8c82af4ad4f988425422c71e8c0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f08bd63baaf34ccca18fb6ff9eddd93f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.14M/1.14M [00:01&lt;00:00, 773kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d605346f92244bf9067284a10a83d42"
          }
        },
        "405bd06719854c1380c38cb71c3ec9aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7bf8f690f2a443ccbe2d253c1e7cabca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f08bd63baaf34ccca18fb6ff9eddd93f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d605346f92244bf9067284a10a83d42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j5ce7RMn9p2",
        "outputId": "0610693a-38c5-41d2-9c44-0970dbec2e3d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 12 03:44:09 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqZWENSooKqr",
        "outputId": "4f1c9b7d-4880-4277-857a-c236c35e7fbd"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 13.4MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 18.4MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 17.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 40kB 15.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 8.2MB/s eta 0:00:01\r\u001b[K     |█                               | 81kB 7.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 133kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 143kB 8.6MB/s eta 0:00:01\r\u001b[K     |██                              | 153kB 8.6MB/s eta 0:00:01\r\u001b[K     |██                              | 163kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 174kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 184kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 194kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 204kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 215kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 225kB 8.6MB/s eta 0:00:01\r\u001b[K     |███                             | 235kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 245kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 256kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 266kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 276kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 286kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 296kB 8.6MB/s eta 0:00:01\r\u001b[K     |████                            | 307kB 8.6MB/s eta 0:00:01\r\u001b[K     |████                            | 317kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 327kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 337kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 348kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 358kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 368kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 378kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 389kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 399kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 409kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 419kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 430kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 440kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 450kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 460kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 471kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 481kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 491kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 501kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 512kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 522kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 532kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 542kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 552kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 563kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 573kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 583kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 593kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 604kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 614kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 624kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 634kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 645kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 655kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 665kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 675kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 686kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 696kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 706kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 716kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 727kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 737kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 747kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 757kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 768kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 778kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 788kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 798kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 808kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 819kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 829kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 839kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 849kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 860kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 870kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 880kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 890kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 901kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 911kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 921kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 931kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 942kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 952kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 962kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 972kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 983kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 993kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.7MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.8MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.9MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 34.8MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cydWvI-ioKtb",
        "outputId": "b0a344bf-c342-4163-bae0-8336416d10a5"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o_uTSjgoKv6"
      },
      "source": [
        "import torch \n",
        "from transformers import BertModel,BertConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,get_constant_schedule\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch import nn,optim \n",
        "import seaborn as sns \n",
        "import numpy as np \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,precision_score,recall_score,f1_score\n",
        "# from fairseq.models.roberta import RobertaModel\n",
        "# from fairseq.data.encoders.fastbpe import fastBPE\n",
        "import random\n",
        "import os \n",
        "from matplotlib import pyplot as plt \n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_fSG7dBoKy_"
      },
      "source": [
        "!cp /content/drive/MyDrive/Viettel3/train_n.csv /content\n",
        "!cp /content/drive/MyDrive/Viettel3/test_n.csv /content"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsokP0EYoK1a"
      },
      "source": [
        "train=pd.read_csv('train_n.csv')\n",
        "test=pd.read_csv('test_n.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "be17ff7b36b24d108c0582308a892914",
            "bf4a06bdabfc428996f5d31eee2f3672",
            "7314357dc03944b596e771c645c70b90",
            "8d8ffaaeced04692a4844481bac1351d",
            "1972e9e6a69f468f8444cde1d74d298d",
            "ff8a6bec9adf4b41a029b9b35b59734c",
            "19d5d673346f41e5855d4a138a95be6a",
            "aa0d075a5f154b21979a572a62d21b22",
            "bb97bca2b8c04620a0f12c4338847a63",
            "b67ad8c5c148422a979bb883fb97b931",
            "6fd2fe40dd274e7db9842ae91f83b31f",
            "0b68efa816844299a0cc425f9b82793d",
            "dc776ee590394dc8b31652ea3684433b",
            "4ba9749fa8f74e5b8b742b7a383fb8f4",
            "a0cc5516341640379d7f2507ed6b26a3",
            "a9ab8349d7f243619bf9aa64602186b0",
            "fa4f5189f341468990b240395a1aef47",
            "874b2b6f092449d3ab0f357ec076a9ea",
            "82757dd96fc6482886dbe7bb8ce824dd",
            "846f4f79cdee41ebaec9ae22c671f8d6",
            "8be834c52eb14221a6daa2b0ae6b05f2",
            "852aac3edecd429899c947966252664e",
            "6443d0372e664849b0576e543f6ec3c2",
            "6d95e41450a2411796758a055087e733",
            "26db5ac105514318a034a4f3e8a329fc",
            "ef4a537c04c34f98bd9012870cf15e19",
            "92ca5802b14844f18f17128836b83541",
            "30c8c82af4ad4f988425422c71e8c0ba",
            "405bd06719854c1380c38cb71c3ec9aa",
            "7bf8f690f2a443ccbe2d253c1e7cabca",
            "f08bd63baaf34ccca18fb6ff9eddd93f",
            "5d605346f92244bf9067284a10a83d42"
          ]
        },
        "id": "Zdt2fJHjoK6U",
        "outputId": "c5b28238-8d30-4b4b-96a9-0e03f00a1452"
      },
      "source": [
        "phobert=AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be17ff7b36b24d108c0582308a892914",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=557.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb97bca2b8c04620a0f12c4338847a63",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=542923308.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa4f5189f341468990b240395a1aef47",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=895321.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26db5ac105514318a034a4f3e8a329fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1135173.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkKOn-MgoK9r"
      },
      "source": [
        "val=pd.DataFrame()\n",
        "random.seed(42)\n",
        "n_classes=len(set(train.label))\n",
        "for i in range(n_classes):\n",
        "    temp=train[train.label==i]\n",
        "    temp=temp.sample(int(len(temp)*0.2))\n",
        "    val=pd.concat([val,temp],axis=0)\n",
        "\n",
        "train=train[~train.index.isin(val.index)]\n",
        "train.to_csv(\"train.csv\",index=False)\n",
        "val.to_csv(\"val.csv\",index=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOQIIYwhfwQY",
        "outputId": "5134284f-f21f-4d31-9568-f331cc741cda"
      },
      "source": [
        "print(len(train))\n",
        "print(len(val))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27012\n",
            "6747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPwoX60IoLAV"
      },
      "source": [
        "!cp /content/train.csv /content/drive/MyDrive/Viettel3/Bert1\n",
        "!cp /content/val.csv /content/drive/MyDrive/Viettel3/Bert1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPYi45n9QPZt"
      },
      "source": [
        "# !cp /content/drive/MyDrive/Viettel3/Bert/val.csv /content\n",
        "# !cp /content/drive/MyDrive/Viettel3/Bert/train.csv /content"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f-OCreNoLC7"
      },
      "source": [
        "train=pd.read_csv('train.csv')\n",
        "val=pd.read_csv('val.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujwmd5DxoLFy"
      },
      "source": [
        "labels={\n",
        "0:\"Van hoa\",\n",
        "1:\"Chinh tri Xa hoi\",\n",
        "2:\"Khoa hoc\",\n",
        "3:\"Phap luat\",\n",
        "4:\"Vi tinh\",\n",
        "5:\"Suc khoe\",\n",
        "6:\"Kinh doanh\",\n",
        "7:\"The thao\",\n",
        "8:\"The gioi\",\n",
        "9:\"Doi song\"\n",
        "}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFPKNPwhoLIt",
        "outputId": "c0aed207-854a-4bd8-89bc-f54385c4863a"
      },
      "source": [
        "max_sequence_length=256\n",
        "\n",
        "def encode_data(data):\n",
        "  result=[]\n",
        "  for text in data.text.values:\n",
        "    temp=tokenizer.encode(text)\n",
        "    if len(temp)<max_sequence_length:\n",
        "      temp+=[1]*(max_sequence_length-len(temp))\n",
        "    else:\n",
        "      t1=temp[:64]\n",
        "      t2=temp[-192:]\n",
        "      temp=t1+t2\n",
        "    result.append(temp)\n",
        "  return result\n",
        "\n",
        "train['data']=encode_data(train)\n",
        "test['data']=encode_data(test)\n",
        "val['data']=encode_data(val)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (325 > 256). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbZr5mHHoLLk"
      },
      "source": [
        "train_dataset_tensor=torch.utils.data.TensorDataset(torch.tensor(train['data'],dtype=torch.long),torch.tensor(train['label'],dtype=torch.long))\n",
        "test_dataset_tensor=torch.utils.data.TensorDataset(torch.tensor(test['data'],dtype=torch.long),torch.tensor(test['label'],dtype=torch.long))\n",
        "val_dataset_tensor=torch.utils.data.TensorDataset(torch.tensor(val['data'],dtype=torch.long),torch.tensor(val['label'],dtype=torch.long))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zXVassdoLO9"
      },
      "source": [
        "batch_size=64\n",
        "train_loader=DataLoader(train_dataset_tensor,batch_size=batch_size,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset_tensor,batch_size=batch_size,shuffle=False)\n",
        "val_loader=DataLoader(val_dataset_tensor,batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqiVFJgnoLSt",
        "outputId": "0d3b9b08-e7c8-4cb3-b492-a8ea99b4cf01"
      },
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(val_loader))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "423\n",
            "788\n",
            "106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CljGpALJoLU6"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self,n_classes,drop_prob=0.2):\n",
        "    super(Model,self).__init__()\n",
        "    self.n_classes=n_classes\n",
        "    self.bert=phobert\n",
        "    self.linear=nn.Sequential(\n",
        "        nn.Linear(self.bert.config.hidden_size,512),\n",
        "        nn.LayerNorm(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,self.n_classes)\n",
        "    )\n",
        "    self.norm=nn.LayerNorm(self.bert.config.hidden_size)\n",
        "      \n",
        "  def forward(self,x_batch):\n",
        "    cls_embedding = self.bert(x_batch)[1]\n",
        "    input_linear=self.norm(cls_embedding)\n",
        "    out=self.linear(input_linear)\n",
        "    return out\n",
        "\n",
        "class LabelSmoothingCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self,smoothing_value=0.04,reduction='mean'):\n",
        "        super(LabelSmoothingCrossEntropyLoss,self).__init__()\n",
        "        self.smoothing_value=smoothing_value\n",
        "        self.reduction=reduction\n",
        "    \n",
        "    def reduce_loss(self,loss,reduction='mean'):\n",
        "        return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss \n",
        "\n",
        "    def forward(self,outputs,targets):\n",
        "        n_classes=outputs.size(1)\n",
        "        log_preds=F.log_softmax(outputs,dim=1)\n",
        "        loss=self.reduce_loss(-log_preds.sum(dim=-1),self.reduction)\n",
        "        nll=F.nll_loss(log_preds,targets,reduction=self.reduction)\n",
        "        return (1-self.smoothing_value)*nll+ self.smoothing_value*(loss/n_classes)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMvUFZkO0DNz"
      },
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLQa-pM1oLXx"
      },
      "source": [
        "n_epochs=100\n",
        "num_classes=len(set(train.label))\n",
        "model=Model(num_classes).to(device)\n",
        "optimizer=AdamW(model.parameters(),lr=1e-5)\n",
        "scheduler=get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=100,\n",
        "    num_training_steps=len(train_loader)*(n_epochs-1)\n",
        ")\n",
        "scheduler_frozen=get_constant_schedule(optimizer)\n",
        "criterion=LabelSmoothingCrossEntropyLoss().to(device)\n",
        "frozen=True"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZcBHhrZoLaT"
      },
      "source": [
        "for child in phobert.children():\n",
        "  for param in child.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AsrcerxoLf1"
      },
      "source": [
        "def train_model(model,data_loader,scheduler_model):\n",
        "  print(\"-----------------------Training--------------------------\")\n",
        "  model.train()\n",
        "  loss_train=0\n",
        "  final_preds=[]\n",
        "  final_true=[]\n",
        "  for idx,(x_batch,y_batch) in enumerate(data_loader):    \n",
        "    x_batch=x_batch.to(device)\n",
        "    y_batch=y_batch.to(device)\n",
        "    outputs=model(x_batch)\n",
        "    loss=criterion(outputs,y_batch)\n",
        "    outputs=torch.softmax(outputs,dim=1)\n",
        "    preds=torch.argmax(outputs,dim=1)\n",
        "    final_preds.extend(preds.cpu().numpy().tolist())\n",
        "    final_true.extend(y_batch.cpu().numpy().tolist())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler_model.step()\n",
        "    loss_train+=loss.item()\n",
        "    if idx%50==0:\n",
        "      print(idx,end=' ')\n",
        "  print()  \n",
        "  return accuracy_score(final_true,final_preds),loss_train/len(data_loader)\n",
        "\n",
        "\n",
        "def evaluate(model,data_loader):\n",
        "  print(\"------------------------Evaluate---------------------------\")\n",
        "  model.eval()\n",
        "  loss_val=0\n",
        "  final_preds=[]\n",
        "  final_true=[]\n",
        "  for idx,(x_batch,y_batch) in enumerate(data_loader):\n",
        "    x_batch=x_batch.to(device)\n",
        "    y_batch=y_batch.to(device)\n",
        "    outputs=model(x_batch)\n",
        "    loss=criterion(outputs,y_batch)\n",
        "    outputs=torch.softmax(outputs,dim=1)\n",
        "    preds=torch.argmax(outputs,dim=1)\n",
        "    loss_val+=loss.item()\n",
        "    final_preds.extend(preds.cpu().numpy().tolist())\n",
        "    final_true.extend(y_batch.cpu().numpy().tolist())\n",
        "    if idx%50==0:\n",
        "      print(idx,end=' ')\n",
        "\n",
        "  print()\n",
        "  return accuracy_score(final_true,final_preds),loss_val/len(data_loader)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_DDL831or8Q"
      },
      "source": [
        "def testing(data_loader):\n",
        "  model.eval()\n",
        "  final_preds=[]\n",
        "  final_true=[]\n",
        "  with torch.no_grad():\n",
        "    for idx,(x_batch,y_batch) in enumerate(data_loader):\n",
        "      x_batch=x_batch.to(device)\n",
        "      y_batch=y_batch.to(device)\n",
        "      outputs=model(x_batch)\n",
        "      outputs=torch.softmax(outputs,dim=1)\n",
        "      preds=torch.argmax(outputs,dim=1)\n",
        "      final_preds.extend(preds.cpu().numpy().tolist())\n",
        "      final_true.extend(y_batch.cpu().numpy().tolist())\n",
        "      if idx%50==0:\n",
        "        print(idx,end=' ')\n",
        "  print()\n",
        "  print(\"Accuracy for test:\",accuracy_score(final_true,final_preds))\n",
        "\n",
        "def save_checkpoint(epoch,scheduler_model):\n",
        "  path='/content/drive/MyDrive/Viettel3/Bert1'\n",
        "  if os.path.exists(path) is False:\n",
        "    os.mkdir(path)\n",
        "  \n",
        "  with open('/content/drive/MyDrive/Viettel3/Bert1/history{}.pickle'.format(epoch),'wb') as file:\n",
        "    pickle.dump(history,file)\n",
        "  \n",
        "  print(\"Save history done\")\n",
        "  model_state={\n",
        "      'model':model.state_dict(),\n",
        "      'optimizer':optimizer.state_dict(),\n",
        "      'scheduler':scheduler_model.state_dict(),\n",
        "      'loss':criterion\n",
        "  }\n",
        "  torch.save(model_state,path+'/model{}.pth'.format(epoch))\n",
        "  print(\"Save model done\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wf8y106wor_r",
        "outputId": "9b430312-9136-4b87-fedd-94a8a519ea3f"
      },
      "source": [
        "from collections import defaultdict\n",
        "import time \n",
        "history=defaultdict(list)\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  if epoch > 0 and frozen:\n",
        "    for child in model.bert.children():\n",
        "      for param in child.parameters():\n",
        "        param.requires_grad = False\n",
        "    del scheduler_frozen\n",
        "    torch.cuda.empty_cache()\n",
        "    frozen=False \n",
        "\n",
        "  start_time=time.time()\n",
        "  if frozen:\n",
        "    train_acc,train_loss=train_model(model,train_loader,scheduler_frozen)\n",
        "  else:\n",
        "    train_acc,train_loss=train_model(model,train_loader,scheduler)\n",
        "    \n",
        "  val_acc,val_loss=evaluate(model,val_loader)\n",
        "  print(f\"Epoch {epoch}----Train acc:{train_acc}----Train loss:{train_loss}---Val acc:{val_acc}---Val loss:{val_loss}---Time:{time.time()-start_time}\")\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  if epoch>8 and epoch%3==0:\n",
        "    print(\"--------------Time for testing-----------------\")\n",
        "    testing(test_loader) \n",
        "    save_checkpoint(epoch,scheduler)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 0----Train acc:0.31915444987413----Train loss:2.0553463347978345---Val acc:0.49592411442122425---Val loss:1.7411242633495692---Time:262.85538506507874\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 1----Train acc:0.537946098030505----Train loss:1.6854851668608104---Val acc:0.6198310360160071---Val loss:1.4339915750161656---Time:262.6916482448578\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 2----Train acc:0.6205760402783947----Train loss:1.4464535346831553---Val acc:0.6746702238031718---Val loss:1.2588489977818615---Time:262.64486265182495\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 3----Train acc:0.6514512068710203----Train loss:1.3131022353262485---Val acc:0.6909737661182748---Val loss:1.1696249212858811---Time:262.65142130851746\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 4----Train acc:0.6745150303568784----Train loss:1.2255587365046743---Val acc:0.7112790870016303---Val loss:1.0870786191157575---Time:262.65472507476807\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 5----Train acc:0.6860284318080853----Train loss:1.1686486190374297---Val acc:0.7262487031273158---Val loss:1.0377382056893043---Time:262.6536147594452\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 6----Train acc:0.6950984747519621----Train loss:1.1236149630648025---Val acc:0.7323254779902179---Val loss:1.0021467248223863---Time:262.65078687667847\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 7----Train acc:0.7022804679401747----Train loss:1.090922075524116---Val acc:0.7379576107899807---Val loss:0.9759906659711082---Time:262.75290417671204\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 8----Train acc:0.7098696875462758----Train loss:1.06177318490707---Val acc:0.7477397361790425---Val loss:0.9447864085998176---Time:262.8184494972229\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 9----Train acc:0.715422775062935----Train loss:1.0354168174114633---Val acc:0.7469986660738106---Val loss:0.9368312370102361---Time:262.7896535396576\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.762412403470113\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 10----Train acc:0.7201614097438176----Train loss:1.0166013883360734---Val acc:0.7541129390840373---Val loss:0.9129287544286476---Time:262.8091812133789\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 11----Train acc:0.7234192210869244----Train loss:1.0033053579623536---Val acc:0.7557432933155477---Val loss:0.9077482577764763---Time:262.7945213317871\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 12----Train acc:0.7294165556049164----Train loss:0.9878353617433678---Val acc:0.750555802578924---Val loss:0.9037811986680301---Time:262.7854542732239\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.7734500625335001\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 13----Train acc:0.7271583000148082----Train loss:0.9796885966409182---Val acc:0.7576700755891508---Val loss:0.89092296699308---Time:262.7800455093384\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 14----Train acc:0.7344513549533541----Train loss:0.9694511232364826---Val acc:0.7578182896101971---Val loss:0.884297472126079---Time:262.7837769985199\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 15----Train acc:0.7363023841255738----Train loss:0.9612239318818347---Val acc:0.7649325626204239---Val loss:0.8666495928224528---Time:262.7682955265045\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.7879816568399738\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 16----Train acc:0.7378572486302384----Train loss:0.9490695206831533---Val acc:0.7677486290203053---Val loss:0.8604749165615946---Time:262.77849221229553\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 17----Train acc:0.74326225381312----Train loss:0.9390676769804447---Val acc:0.7656736327256558---Val loss:0.8570931328917449---Time:262.78033781051636\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 18----Train acc:0.7396712572190137----Train loss:0.9416762473453585---Val acc:0.7677486290203053---Val loss:0.8540694634869413---Time:262.76623368263245\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.7875052111250075\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 19----Train acc:0.745964756404561----Train loss:0.9257416137566803---Val acc:0.7754557581147177---Val loss:0.835630791367225---Time:262.565637588501\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 20----Train acc:0.7465570857396713----Train loss:0.9212649139106697---Val acc:0.7769378983251816---Val loss:0.8325612443797993---Time:262.5272226333618\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 21----Train acc:0.7480008884940027----Train loss:0.9200132279249511---Val acc:0.7784200385356455---Val loss:0.8324786503359957---Time:262.61407923698425\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.7973120520913982\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 22----Train acc:0.7484451354953354----Train loss:0.9126067733651921---Val acc:0.7793093226619238---Val loss:0.8273027460530119---Time:262.5952033996582\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 23----Train acc:0.7507774322523323----Train loss:0.9043479740760568---Val acc:0.7754557581147177---Val loss:0.8335057080916639---Time:262.59604692459106\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 24----Train acc:0.7492595883311121----Train loss:0.9101487943466674---Val acc:0.7785682525566918---Val loss:0.8274495787215683---Time:262.62444496154785\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.7955253806602743\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 25----Train acc:0.754775655264327----Train loss:0.896909545781201---Val acc:0.7800503927671558---Val loss:0.8280771638987199---Time:262.83402395248413\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 26----Train acc:0.754960758181549----Train loss:0.8986824215444831---Val acc:0.7853860975248258---Val loss:0.8142829379945431---Time:262.80459856987\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 27----Train acc:0.7505182881682215----Train loss:0.902775272128148---Val acc:0.7828664591670372---Val loss:0.8163586883050091---Time:262.7946674823761\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.8002501340003574\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 28----Train acc:0.75673774618688----Train loss:0.8948409660206337---Val acc:0.7883503779457537---Val loss:0.8014788183401216---Time:262.8108835220337\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 29----Train acc:0.7573300755219903----Train loss:0.8858026408416442---Val acc:0.7847932414406403---Val loss:0.8118948902723924---Time:262.78564453125\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 30----Train acc:0.7590330223604324----Train loss:0.885034739125705---Val acc:0.7882021639247073---Val loss:0.806263157219257---Time:262.73697686195374\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.8028110297183014\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 31----Train acc:0.7561454168517696----Train loss:0.8873967285979161---Val acc:0.7873128797984289---Val loss:0.8036502059900535---Time:262.8036034107208\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 32----Train acc:0.7646971716274249----Train loss:0.8762955206223977---Val acc:0.7847932414406403---Val loss:0.8095457295201859---Time:262.76602363586426\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 33----Train acc:0.7580334666074338----Train loss:0.8814752390762312---Val acc:0.7896843041351712---Val loss:0.8021941837274803---Time:262.7448697090149\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.8078931173446092\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 34----Train acc:0.7626240189545387----Train loss:0.8772249359892897---Val acc:0.7874610938194754---Val loss:0.8018241918311929---Time:262.77305793762207\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 35----Train acc:0.7633274100399823----Train loss:0.8781993466629768---Val acc:0.7930932266192382---Val loss:0.7860506744879596---Time:262.76510667800903\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 36----Train acc:0.7635125129572042----Train loss:0.868155577926771---Val acc:0.7935378686823773---Val loss:0.7856449316132743---Time:262.76411032676697\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.8117245349691303\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 37----Train acc:0.7638827187916482----Train loss:0.8697515156815802---Val acc:0.7925003705350526---Val loss:0.790775384543077---Time:262.80010056495667\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 38----Train acc:0.7672886124685325----Train loss:0.8626932064973998---Val acc:0.7896843041351712---Val loss:0.7941518510287663---Time:262.7286238670349\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 39----Train acc:0.7678809418036429----Train loss:0.8632864461722949---Val acc:0.7941307247665629---Val loss:0.7840326726436615---Time:262.4946279525757\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.8130149087804975\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 40----Train acc:0.7696949503924182----Train loss:0.8562235651850418---Val acc:0.7947235808507485---Val loss:0.7871595491778176---Time:262.6678729057312\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 41----Train acc:0.7712868354805271----Train loss:0.8569390219999543---Val acc:0.7966503631243516---Val loss:0.7806236479642257---Time:262.4707980155945\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 42----Train acc:0.769583888642085----Train loss:0.8600891550945615---Val acc:0.7982807173558618---Val loss:0.7763324661074944---Time:262.44998049736023\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.8154566930697\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 43----Train acc:0.7702502591440841----Train loss:0.853321528744754---Val acc:0.7970950051874908---Val loss:0.7792172735592104---Time:262.50580859184265\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 44----Train acc:0.7668443654671998----Train loss:0.8600290544489597---Val acc:0.7951682229138877---Val loss:0.7803874218238974---Time:262.44296741485596\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 45----Train acc:0.7659928920479787----Train loss:0.8588320149315728---Val acc:0.7973914332295835---Val loss:0.7752361618122965---Time:262.45190358161926\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.8142060230679133\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 46----Train acc:0.7676958388864209----Train loss:0.8552816462291329---Val acc:0.7975396472506299---Val loss:0.7810518724738427---Time:262.46939301490784\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 47----Train acc:0.7668813860506442----Train loss:0.855433962446578---Val acc:0.7965021491033052---Val loss:0.7776415184983667---Time:262.4420762062073\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 48----Train acc:0.772767658818303----Train loss:0.8476612638356275---Val acc:0.7976878612716763---Val loss:0.7745040655136108---Time:262.4351694583893\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.8139876521152205\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 49----Train acc:0.7684732711387532----Train loss:0.8529881644756236---Val acc:0.7991700014821402---Val loss:0.769772616759786---Time:262.63297629356384\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 50----Train acc:0.7699911150599733----Train loss:0.8497535610311702---Val acc:0.7959092930191196---Val loss:0.7786674994342732---Time:262.6543037891388\n",
            "-----------------------Training--------------------------\n",
            "0 50 100 150 200 250 300 350 400 \n",
            "------------------------Evaluate---------------------------\n",
            "0 50 100 \n",
            "Epoch 51----Train acc:0.7735450910706353----Train loss:0.8405132731929175---Val acc:0.7966503631243516---Val loss:0.7759765127919754---Time:262.76846265792847\n",
            "--------------Time for testing-----------------\n",
            "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 \n",
            "Accuracy for test: 0.813352391161932\n",
            "Save history done\n",
            "Save model done\n",
            "-----------------------Training--------------------------\n",
            "0 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c72d211998f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler_frozen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-b0e81d88cf97>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, scheduler_model)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mx_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-e77ca82242e2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_batch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcls_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0minput_linear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_linear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    847\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    527\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m                 )\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         )\n\u001b[1;32m    416\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLRfJiLJhR9B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab43R3oK0Ji-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3DVPb-T0KI1"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1T8ipNV0Jlk",
        "outputId": "da9efd98-e098-4716-b8d5-200141822e9f"
      },
      "source": [
        "model_state=torch.load('/content/drive/MyDrive/Viettel3/Bert/model21.pth')\n",
        "model=Model(n_classes=10).to(device)\n",
        "model.load_state_dict(model_state['model'])\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (bert): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (lstm): LSTM(768, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
              "  (linear): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
              "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_tnzDov0jjp"
      },
      "source": [
        "def evaluate(data_loader):\n",
        "  final_preds=[]\n",
        "  final_true=[]\n",
        "  for idx,(x_batch,y_batch) in enumerate(data_loader):\n",
        "    x_batch=x_batch.to(device)\n",
        "    y_batch=y_batch.to(device)\n",
        "    outputs=model(x_batch)\n",
        "    outputs=torch.softmax(outputs,dim=1)\n",
        "    preds=torch.argmax(outputs,dim=1)\n",
        "    final_preds.extend(preds.cpu().numpy().tolist())\n",
        "    final_true.extend(y_batch.cpu().numpy().tolist())\n",
        "    if idx%100==0:\n",
        "      print(idx,end=' ')\n",
        "  return final_true,final_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osJlgpww03F1",
        "outputId": "4cad020c-c7bc-4fa8-e09e-8936742b6c80"
      },
      "source": [
        "final_true_val,final_preds_val=evaluate(val_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 100 200 300 400 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD3OzdwG1qLX",
        "outputId": "4a557929-a089-4907-df52-c0c8e8d84af2"
      },
      "source": [
        "final_true_test,final_true_preds=evaluate(test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMPXivJp2DR1"
      },
      "source": [
        "def show_result(y_true,y_preds):\n",
        "  print(accuracy_score(y_true,y_preds))\n",
        "  print(precision_score(y_true,y_preds,average='weighted'))\n",
        "  print(recall_score(y_true,y_preds,average='weighted'))\n",
        "  print(f1_score(y_true,y_preds,average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcjIZ4jg3OLQ",
        "outputId": "c9467332-05e8-4587-962a-9add5d0a7814"
      },
      "source": [
        "print(classification_report(final_true_val,final_preds_val,target_names=list(labels.values())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "         Van hoa       0.95      0.93      0.94       616\n",
            "Chinh tri Xa hoi       0.87      0.81      0.84      1043\n",
            "        Khoa hoc       0.91      0.79      0.85       364\n",
            "       Phap luat       0.92      0.93      0.93       773\n",
            "         Vi tinh       0.96      0.95      0.95       496\n",
            "        Suc khoe       0.89      0.97      0.93       676\n",
            "      Kinh doanh       0.82      0.92      0.87       510\n",
            "        The thao       0.99      0.99      0.99      1059\n",
            "        The gioi       0.95      0.90      0.92       579\n",
            "        Doi song       0.85      0.91      0.88       631\n",
            "\n",
            "        accuracy                           0.91      6747\n",
            "       macro avg       0.91      0.91      0.91      6747\n",
            "    weighted avg       0.91      0.91      0.91      6747\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBiI0TB33maH",
        "outputId": "2449c324-4595-463a-ed21-3fbcdda436be"
      },
      "source": [
        "print(classification_report(final_true_test,final_true_preds,target_names=list(labels.values())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "         Van hoa       0.93      0.95      0.94      6250\n",
            "Chinh tri Xa hoi       0.89      0.85      0.87      7567\n",
            "        Khoa hoc       0.85      0.75      0.80      2096\n",
            "       Phap luat       0.89      0.92      0.91      3788\n",
            "         Vi tinh       0.94      0.95      0.94      4560\n",
            "        Suc khoe       0.91      0.95      0.93      5417\n",
            "      Kinh doanh       0.87      0.92      0.89      5276\n",
            "        The thao       0.98      0.99      0.98      6667\n",
            "        The gioi       0.96      0.92      0.94      6716\n",
            "        Doi song       0.72      0.70      0.71      2036\n",
            "\n",
            "        accuracy                           0.91     50373\n",
            "       macro avg       0.89      0.89      0.89     50373\n",
            "    weighted avg       0.91      0.91      0.91     50373\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N59k9HoK2z8o",
        "outputId": "b2d7d1b0-2495-44a3-dd25-6e4bb82df339"
      },
      "source": [
        "show_result(final_true_test,final_true_preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9122148770174499\n",
            "0.9119381121434791\n",
            "0.9122148770174499\n",
            "0.9116690540717909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV3PPkDU3F9u"
      },
      "source": [
        "def plot(final_true,final_preds,file_name):\n",
        "  temp=confusion_matrix(final_true,final_preds)\n",
        "  df=pd.DataFrame(temp,index=labels.values(),columns=labels.values()).astype(int)\n",
        "  plt.figure(figsize=(12,8))\n",
        "  heatmap=sns.heatmap(df,annot=True,fmt=\"d\")\n",
        "  heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(),rotation=0,ha='right')\n",
        "  heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(),rotation=45,ha='right')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  result=accuracy_score(final_true,final_preds)\n",
        "  plt.title(\"Accuracy for test:{:0.5f}\".format(result))\n",
        "  plt.savefig('/content/drive/MyDrive/Viettel3/Bert/{}'.format(file_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiPoqPop3sbk"
      },
      "source": [
        "plot(final_true_val,final_preds_val,'bert_val.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x-P4j6X31Uu"
      },
      "source": [
        "plot(final_true_test,final_true_preds,'bert_test.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}